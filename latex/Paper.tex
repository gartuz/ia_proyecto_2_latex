\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

\usepackage{fontspec} % For XeLaTeX, allows full UTF-8 support and custom fonts
\setmainfont{TeX Gyre Termes} % Times-compatible font with good support for different font shapes

\usepackage{cite}
\bibliographystyle{plain} % Choose a bibliography style
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{url} % Ensures URLs are handled correctly
\usepackage[english]{babel} % Set language to English for consistency with content
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{A Comparative Study on Machine Learning Models Performance}

\author{
    \scalebox{0.8}{%
        \begin{tabular}{c@{\hspace{1.5em}}c@{\hspace{1.5em}}c@{\hspace{1.5em}}c@{\hspace{1.5em}}c}
            &
            \begin{tabular}[t]{c}
                Alexander Sánchez Zamora \\
                \small\itshape ECCI, UCR \\
                \small alexander.sanchezzamora@ucr.ac.cr
            \end{tabular}
            &
            \begin{tabular}[t]{c}
                Jean Paul Chacón González \\
                \small\itshape ECCI, UCR \\
                \small jean.chacongonzalez@ucr.ac.cr
            \end{tabular}
            &
            \begin{tabular}[t]{c}
                Gabriel Martinez Cruz \\
                \small\itshape ECCI, UCR \\
                \small gabriel.martinez@ucr.ac.cr
            \end{tabular}
            &
            \begin{tabular}[t]{c}
                Gabriel Molina Bulgarelli \\
                \small\itshape ECCI, UCR \\
                \small gabriel.molinabulgarelli@ucr.ac.cr
            \end{tabular}
        \end{tabular}%
    }
}

\maketitle

\section{Abstract}



\begin{IEEEkeywords}
Machine Learning, Linear Regression, Decision Tree, KNN, MLP
\end{IEEEkeywords}

\section{Introduction}

Artificial intelligence has transformed the world in numerous ways since its emergence over 20 years ago. One of the most prominent branches within this field is Machine Learning, which focuses on developing learning models capable of adapting to and analyzing data at a deeper level than traditional algorithms. Over the years, many significant models have been developed and studied. Among them are Linear Regression (LR) for prediction and its counterpart, Logistic Regression, for classification. Both are among the most basic models, using a linear function that, by adjusting the weights of its variables, enables estimation or classification of data.

Another notable model is K-Nearest Neighbors (KNN), which, as the name suggests, makes decisions based on the nearest instances to the data being processed. This approach is particularly useful for non-linear data, which can pose challenges for Linear Regression.

In the realm of low- to moderate-complexity models, the Decision Tree (DT) offers flexibility comparable to or greater than KNN, with a higher number of hyperparameters that allow customization and adaptation to different datasets.

Finally, at higher levels of complexity, we enter the domain of Neural Networks (NN). These networks employ various methods and strategies to analyze and understand data, with the simplest form being the Multilayer Perceptron (MLP). MLPs are composed of multiple fully connected layers of perceptrons, each of which processes a portion of the information and passes it to the next layer, enabling in-depth data processing.
    
To effectively use these models, it is essential to train them with a sufficient amount of data to identify patterns and accurately predict outcomes. Numerous datasets are available for this purpose. One of the most well-known is the red wine quality dataset, which classifies various wines into quality levels based on parameters such as acidity and alcohol content, among others.

There are also more specialized datasets, such as those used in disease research, which are typically more complex and domain-specific. This study focuses on analyzing and comparing the performance and behavior of the aforementioned models using two different datasets.

The paper structure is the following: First the Methods, where there is an analysis on the data and its processing before implementing the methods. This section also specifies the different hyperparameter values used in each model. Then there are the results of the experiments, where a comparison of performances is made. Following is the discussion, where these results are analysed and finally the conclussion of the study.

\section{Methods}
This paper employs a machine learning approach for binary classification. The four distinct machine learning methods that are going to be implemented, as mentioned previously: Linear Regression, K-Nearest Neighbors, Decision Tree, and MultiLayer Perceptron model.

\subsection{Data Collection}
\subsubsection{Dataset} 
This study uses two different datasets. The first dataset is the Red Wine Quality (RWQ) dataset which consists of 1599 records and 11 features, encompassing chemical factors related to red wine quality. Preprocessing involved standardizing and normalizing numerical features, encoding categorical variables, and balancing the classes using the Random Over-sampling Technique (ROS) to address class imbalance in the 'quality' variable. This approach ensured fair representation and minimized the risk of model bias \cite{wine_quality}.

\subsubsection{\textbf{Parameters}} The following parameters will be extracted from the RWQ dataset and used for model training and evaluation:

    \begin{itemize}
    \item \textbf{Fixed acidity:} amount of acids that do not evaporate readily.
    \item \textbf{Volatile acidity:}  the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste.
    \item \textbf{Citric acid:} amount of citric acid found in the wine.
    \item \textbf{Residual sugar:} the amount of sugar remaining after fermentation stops.
    \item \textbf{Chlorides:} the amount of salt in the wine.
    \item \textbf{Free sulfur dioxide:} the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas).
    \item \textbf{Total sulfur dioxide:} amount of free and bound forms of S02.
    \item \textbf{Density:} wine density depending on the percent alcohol and sugar content.
    \item \textbf{pH:}  how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic).
    \item \textbf{Sulphates:} a wine additive which can contribute to sulfur dioxide gas (S02) levels
    \item \textbf{Alcohol:} the percent alcohol content of the wine
    \end{itemize}
    
The wine quality is categorized in a scale from 0 to 10, athough in order to make a binary classification this feature was modified so that every wine of a quality score of 5 or below will be considered "bad" wine and wine above 5 score will be considered "good" wine.

On the other hand, the second dataset used is the Alzheimer's Disease (AD) Dataset, which is an extensive health information for 2,149 patients. The dataset includes demographic details, lifestyle factors, medical history, clinical measurements, cognitive and functional assessments, symptoms, and a diagnosis of Alzheimer's Disease. This dataset has much more complexity than the first one, since this has 34 features, of which most of them are binary. Each instance contains the followin patient information:

    \begin{itemize}
    \item \textbf{Patient ID:} Each patient has a unique 4 digit ID.
    \item \textbf{Demographic Detail:} From each patient the Age, Gender, Ethnicity (Caucasian, African American, Asian or Other) and the Education Level (None, High School, Bachelor or Higher) is registered.
    \item \textbf{Lifestyle Factors:} Physiological data like BMI or Sleep Quality and habits like Smoking, Alcohol Consumption, Physical Activity and Diet Quality.
    \item \textbf{Medical History:} Any diagnosis made that could be relevant like Family  History with Alzheimer (a binary variable), Cardiovascular Disease, Diabetes, Depression, Head Injury or Hypertension.
    \item \textbf{Clinical Measurments:} Like Systolic Blood Pressure, Diastolic Blood Pressure, Total Cholesterol, Low-Density Lipoprotein Cholesterol levels, High-Density Lipoprotein Cholesterol levels and Triglycerides Levels.
    \item \textbf{Cognitive and Functional Assessments:} Mini-Mental State Examination (MIME) score, Functional Assessments, Memory Complaints, Behavioral Problems, Activities of Daily Living (ADL) score.
    \item \textbf{Symptoms:} Alzheimer's Disease symptomps like Confusion, Disorientation, Personality Changes, Dificulty Completing Tasks and Forgetfulness.
    \item \textbf{Diagnosis Information:} Diagnosis over Alzheimer's Disease.
    \item \textbf{Confidential Information:} Confidential information about the doctor in charge, with "XXXConfid" as the value for all patients.
    \end{itemize}
    
\subsection{Data Preprocessing}

To prepare the dataset for analysis, each feature underwent specific transformations to optimize model training. Numerical features were standardized or normalized to equalize their scale, ensuring that models like KNN, which rely on distance-based calculations, were not skewed by large magnitude differences. Categorical features, such as 'Ethnicity' or 'Educational Level' although were already numerically encoded were encoded through binary vectors, to make these variables accessible for NN models. Handling missing values and addressing outliers, particularly in the 'Residual Sugar' and 'Chlorides' features, maintained data integrity and supported reliable model predictions.

\begin{itemize}
\item \textbf{Cleaning:} The dataset was cleaned by handling missing values and outliers. No column had missing values. Outliers in the columns will be addressed by removing values that are beyond three standard deviations from the mean.
   
\item \textbf{Transformation:} The dataset will undergo a transformation process to prepare it for machine learning. Numeric features, except for 'Patient ID' and both prediction columns, will be standardized using scikit-learn's StandardScaler and normalized with MinMaxScaler. This ensures all features have a mean of 0 and a standard deviation of 1, in the case of standarization and comprising the values in a range between 0 and 1 for normalization, both preventing scale domination issues for distance-based models like KNN and allowing optimal learning for MLP. Categorical features, including 'Ethnicity' or 'Educational Level', will be encoded into binary vectors using scikit-learn's OneHotEncoder. 

\item \textbf{Feature Selection and Correlation Analysis:} In both implementations all features were used in the training and testing of the models. This is due to the lack of correlation with the prediction variable, since every relation in both datasets is weak (below 0.3) and very few to no relations were 'moderate' (between 0.3 and 0.7) or 'strong' (greater than 0.7). 

\item \textbf{Oversampling:} To address class imbalance in the target variables ('quality' for RWQ dataset and 'diagnosis' for AD datset), the ROS technique was used. This will be applied only to the training set after the data split to avoid data leakage. 
\end{itemize}

\subsection{Model Implementation}

Each model was implemented using the following specifications:

\begin{itemize}
% * <AlexBeanie> 16:09:20 21 Nov 2024 UTC-0600:
% No añade información relevante, decidir si eliminar
\item \textbf{Logistic Regression (LR):} 
    \begin{itemize}
    \item The LR algorithm was implemented using scikit-learn in Python.
    \item The best implementation was selected using grid search and 5-fold cross-validation.
    \end{itemize}
    
\item \textbf{K-Nearest Neighbors (KNN):} 
    \begin{itemize}
    \item The KNN algorithm was implemented using scikit-learn in Python.
    \item The optimal value for 'k' (number of neighbors) will be determined using grid search and 5-fold cross-validation.
    \item Distance metric used: Euclidean distance (default in scikit-learn).
    \end{itemize}

\item \textbf{Decision Tree (DT):}
    \begin{itemize}
    \item The DT algorithm will be implemented using scikit-learn in Python.
    \item Hyperparameters to be tuned using grid search and 5-fold cross-validation include:
        \begin{itemize}
        \item 'max\_depth': [10, 20, 30] 
        \item 'min\_samples\_split': [10, 20, 30] 
        \item 'min\_samples\_leaf': [8, 16, 32, 64] 
        \item 'max\_features': [5, 8, 'sqrt', 'log2']
        \end{itemize}
    \end{itemize}

\item \textbf{MultiLayer Perceptron (MLP):}
    \begin{itemize}
    \item The MLP algorithm will be implemented using keras and scikit-learn in Python. 
    \item The best architecture for each dataset was found through fine tunning, grid search and 5-fold cross-validation. 
    \end{itemize}
\end{itemize}

Hyperparameter tuning was conducted through grid search combined with 5-fold cross-validation, optimizing the performance of each model. Cross-validation helped construct the best model across different data splits, ensuring generalizability. This systematic approach maximized each model’s predictive capabilities while maintaining robustness.
\subsection{Model Evaluation}

\begin{itemize}

\item \textbf{Training and Testing:} The dataset will be split into training and testing sets using an 80/20 split with stratified sampling. Also, from the training set, 20\% of the data was selected as a Validation set, in the case of MLP.

\item \textbf{Performance Metrics:} Model performance will be evaluated based on:
\begin{enumerate}
\item \textbf{Accuracy:}

This function computes the test accuracy \cite{accuracy_score}. It compares the expected labels with the predicted labels.

\[Accuracy=\frac{Number\ Of\ Correct\ Predictions}{Total\ Number\ Of\ Predictions}\]
\item  \textbf{Precision:} 

It essentially measures the proportion of positive predictions that were truly correct \cite{precision_score}.
\[Precision=\frac{True\ Positives}{True\ Positives\ +\ False\ Positives}\]

\item  \textbf{Recall:}

 It measures the ability of the model to correctly identify all relevant instances within a dataset \cite{recall_score}.

 \[Recall=\frac{True\ Positives}{True\ Positives\ +\ False\ Negatives}\]
 
 \item \textbf{F1-Score:}

 It is the harmonic mean of precision and recall\cite{sklearn_f1_score}. It is calculated using the following formula.
 
 \[F1 = 2 \cdot \frac{\substack{\text{True} \\ \text{Positives}}}{\substack{\text{True} \\ \text{Positives}} + \substack{\text{False} \\ \text{Positives}} + \substack{\text{False} \\ \text{Negatives}}}\]

\end{enumerate}
\item \textbf{Comparison:} The performance of the four models will be compared to identify the most effective model on each dataset.
\end{itemize}

\section{Results}
In the case of the RWQ dataset, the best performing model was MLP, with an accuracy up to 80\% and a ROC-AUC score over 0.86. KNN was very stable overall, while its accuracy is not as high as MLP its ROC-AUC score was very remarkable. Linear Regression had a surprising behavior, with an unusually high accuracy and ROC-AUC. Another interesting case is decision tree, that had the lowest both accuracy and ROC-AUC score. This may be dut to its overfitting.

Table \ref{tab:model_perf_RWQ} provides a detailed comparison of accuracy, precision, recall, and F1-score for the four models for this dataset.

\begin{table}[ht]
\caption{Model Performance Comparison}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Model & Accuracy & Precision & Recall & F1-Score & ROC-AUC\\
\hline
LR & 0.76 & 0.77 & 0.77 & 0.76 & 0.8368\\
KNN & 0.75 & 0.75 & 0.75 & 0.75 & 0.8408\\
DT & 0.72 & 0.73 & 0.73 & 0.72 & 0.7941\\
MLP & 0.79 & 0.79 & 0.79 & 0.79 & 0.8679\\
\hline
\end{tabular}
\label{tab:model_perf_RWQ}
\end{table}

On the other hand, the AD dataset shows different results. In this case, the best performing model is not MLP (which is the second) but Decision Tree. This model obtained an average accuracy and ROC-AUC score of 92\%. All the performance scores for this dataset are included in the Table \ref{tab:model_perf_AD}

\begin{table}[ht]
\caption{Model Performance Comparison}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Model & Accuracy & Precision & Recall & F1-Score & ROC-AUC\\
\hline
LR & 0.82 & 0.74 & 0.76 & - & 0.8045\\
KNN & 0.70 & 0.57 & 0.64 & - & 0.6875\\
DT & 0.92 & 0.88 & 0.89 & - & 0.9150\\
MLP & 0.80 & 0.72 & 0.75 & - & 0.7958\\
\hline
\end{tabular}
\label{tab:model_perf_AD}
\end{table}

\section{Discussion}

In the first dataset, RWQ, the models generally exhibited similar performance, with accuracy within the 70\% range. The MLP model showed the best performance, likely due to its higher processing and learning capacity.

In contrast, this homogeneity was lost in the second dataset, where model performance varied significantly. This variation is particularly evident in the KNN model’s performance, with accuracy dropping from 75\% to 70\%. Although this figure remains acceptable, it becomes noteworthy considering that most models in this dataset maintained accuracy around 80\%. This result could be attributed to KNN’s difficulty in correctly classifying complex data. Since KNN calculates distances between instances based on data features, an increase in the number of features can lead to higher error rates.

On the other hand, the MLP and LR models had similar performance in the second dataset, whereas MLP was clearly superior in the first dataset. This difference is intriguing, as its cause is not immediately evident. It is unlikely that the amount of data is the issue: following the rule of thumb, with around 30 features, at least 300 instances are required, and this dataset has over 2000. Both models exhibit slight overfitting, but not to a degree that significantly impacts performance. A random effect is also unlikely, as the results are averaged across five different runs. Various MLP architectures were tested, all showing similar performance. It seems the MLP model may be struggling to grasp the specific patterns within this dataset, while LR may be better suited due to its simplicity.

Interestingly, the decision tree displayed completely opposite performance across the two datasets. In the first dataset, it was the worst-performing model, with an accuracy of 72\%, while in the second dataset, it achieved scores above 90\%. This outcome could be explained by the second dataset’s greater number of features, enabling each tree node to leverage more data for decision-making at each split.

\section{Conclusion}

Throughout this experiment, four different machine learning models were implemented using two distinct datasets. The results of these implementations reveal interesting behaviors for each model. Logistic Regression, the simplest of all models, performs moderately well when the data complexity is low. While limited, it can be useful, especially for data with a linear trend.

In contrast, KNN shows strong performance with smaller datasets and low-complexity features; however, its performance can easily degrade when the number of features increases. The decision tree model has the opposite challenge, as it demonstrated better performance with a large number of features. However, it tends to overfit on low-complexity data, and mitigating this overfitting was not straightforward.

Lastly, the MLP neural network exhibited variability in performance depending on the dataset, similar to the other models. Overall, it achieved high accuracy across both implementations, though with relatively lower accuracy on the second dataset. Additionally, MLP showed a tendency to overfit when using a minimal number of layers.

In summary, each model’s performance is influenced by the data’s complexity and size. Determining the best fit for a particular dataset may require experimentation to understand which model will perform optimally.

\bibliographystyle{IEEEtran}
\bibliography{references}


\end{document}
